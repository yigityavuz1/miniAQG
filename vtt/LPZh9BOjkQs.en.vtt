WEBVTT
Kind: captions
Language: en

00:00:01.140 --> 00:00:03.976
Imagine you happen across a short movie script that

00:00:03.976 --> 00:00:07.140
describes a scene between a person and their AI assistant.

00:00:07.480 --> 00:00:13.060
The script has what the person asks the AI, but the AI's response has been torn off.

00:00:13.060 --> 00:00:16.980
Suppose you also have this powerful magical machine that can take

00:00:16.980 --> 00:00:20.960
any text and provide a sensible prediction of what word comes next.

00:00:21.500 --> 00:00:25.506
You could then finish the script by feeding in what you have to the machine,

00:00:25.506 --> 00:00:28.368
seeing what it would predict to start the AI's answer,

00:00:28.368 --> 00:00:32.740
and then repeating this over and over with a growing script completing the dialogue.

00:00:33.380 --> 00:00:36.480
When you interact with a chatbot, this is exactly what's happening.

00:00:37.020 --> 00:00:40.701
A large language model is a sophisticated mathematical function

00:00:40.701 --> 00:00:43.980
that predicts what word comes next for any piece of text.

00:00:44.380 --> 00:00:47.402
Instead of predicting one word with certainty, though,

00:00:47.402 --> 00:00:50.920
what it does is assign a probability to all possible next words.

00:00:51.620 --> 00:00:56.800
To build a chatbot, you lay out some text that describes an interaction between a user

00:00:56.800 --> 00:01:02.040
and a hypothetical AI assistant, add on whatever the user types in as the first part of

00:01:02.040 --> 00:01:07.160
the interaction, and then have the model repeatedly predict the next word that such a

00:01:07.160 --> 00:01:12.460
hypothetical AI assistant would say in response, and that's what's presented to the user.

00:01:13.080 --> 00:01:16.214
In doing this, the output tends to look a lot more natural if

00:01:16.214 --> 00:01:19.500
you allow it to select less likely words along the way at random.

00:01:20.140 --> 00:01:23.620
So what this means is even though the model itself is deterministic,

00:01:23.620 --> 00:01:27.100
a given prompt typically gives a different answer each time it's run.

00:01:28.040 --> 00:01:32.332
Models learn how to make these predictions by processing an enormous amount of text,

00:01:32.332 --> 00:01:34.100
typically pulled from the internet.

00:01:34.100 --> 00:01:39.471
For a standard human to read the amount of text that was used to train GPT-3,

00:01:39.471 --> 00:01:44.360
for example, if they read non-stop 24-7, it would take over 2600 years.

00:01:44.720 --> 00:01:47.340
Larger models since then train on much, much more.

00:01:48.200 --> 00:01:51.780
You can think of training a little bit like tuning the dials on a big machine.

00:01:52.280 --> 00:01:56.301
The way that a language model behaves is entirely determined by these

00:01:56.301 --> 00:02:00.380
many different continuous values, usually called parameters or weights.

00:02:01.020 --> 00:02:04.099
Changing those parameters will change the probabilities

00:02:04.099 --> 00:02:07.180
that the model gives for the next word on a given input.

00:02:07.860 --> 00:02:10.727
What puts the large in large language model is how

00:02:10.727 --> 00:02:13.820
they can have hundreds of billions of these parameters.

00:02:15.200 --> 00:02:18.040
No human ever deliberately sets those parameters.

00:02:18.440 --> 00:02:22.643
Instead, they begin at random, meaning the model just outputs gibberish,

00:02:22.643 --> 00:02:26.560
but they're repeatedly refined based on many example pieces of text.

00:02:27.140 --> 00:02:30.656
One of these training examples could be just a handful of words,

00:02:30.656 --> 00:02:34.496
or it could be thousands, but in either case, the way this works is to

00:02:34.496 --> 00:02:38.120
pass in all but the last word from that example into the model and

00:02:38.120 --> 00:02:42.340
compare the prediction that it makes with the true last word from the example.

00:02:43.260 --> 00:02:47.393
An algorithm called backpropagation is used to tweak all of the parameters

00:02:47.393 --> 00:02:51.196
in such a way that it makes the model a little more likely to choose

00:02:51.196 --> 00:02:55.000
the true last word and a little less likely to choose all the others.

00:02:55.740 --> 00:02:58.750
When you do this for many, many trillions of examples,

00:02:58.750 --> 00:03:03.458
not only does the model start to give more accurate predictions on the training data,

00:03:03.458 --> 00:03:07.783
but it also starts to make more reasonable predictions on text that it's never

00:03:07.783 --> 00:03:08.440
seen before.

00:03:09.420 --> 00:03:13.919
Given the huge number of parameters and the enormous amount of training data,

00:03:13.919 --> 00:03:18.880
the scale of computation involved in training a large language model is mind-boggling.

00:03:19.600 --> 00:03:22.285
To illustrate, imagine that you could perform one

00:03:22.285 --> 00:03:25.400
billion additions and multiplications every single second.

00:03:26.060 --> 00:03:29.326
How long do you think it would take for you to do all of the

00:03:29.326 --> 00:03:32.540
operations involved in training the largest language models?

00:03:33.460 --> 00:03:35.039
Do you think it would take a year?

00:03:36.039 --> 00:03:37.960
Maybe something like 10,000 years?

00:03:39.020 --> 00:03:40.800
The answer is actually much more than that.

00:03:41.120 --> 00:03:43.900
It's well over 100 million years.

00:03:45.520 --> 00:03:47.360
This is only part of the story, though.

00:03:47.540 --> 00:03:49.220
This whole process is called pre-training.

00:03:49.500 --> 00:03:52.646
The goal of auto-completing a random passage of text from the

00:03:52.646 --> 00:03:56.200
internet is very different from the goal of being a good AI assistant.

00:03:56.880 --> 00:04:00.080
To address this, chatbots undergo another type of training,

00:04:00.080 --> 00:04:03.760
just as important, called reinforcement learning with human feedback.

00:04:04.480 --> 00:04:07.498
Workers flag unhelpful or problematic predictions,

00:04:07.498 --> 00:04:11.109
and their corrections further change the model's parameters,

00:04:11.109 --> 00:04:14.780
making them more likely to give predictions that users prefer.

00:04:14.780 --> 00:04:18.860
Looking back at the pre-training, though, this staggering amount of

00:04:18.860 --> 00:04:23.120
computation is only made possible by using special computer chips that

00:04:23.120 --> 00:04:27.260
are optimized for running many operations in parallel, known as GPUs.

00:04:28.120 --> 00:04:31.620
However, not all language models can be easily parallelized.

00:04:32.080 --> 00:04:36.817
Prior to 2017, most language models would process text one word at a time,

00:04:36.817 --> 00:04:42.440
but then a team of researchers at Google introduced a new model known as the transformer.

00:04:43.300 --> 00:04:46.745
Transformers don't read text from the start to the finish,

00:04:46.745 --> 00:04:49.140
they soak it all in at once, in parallel.

00:04:49.900 --> 00:04:54.600
The very first step inside a transformer, and most other language models for that matter,

00:04:54.600 --> 00:04:57.420
is to associate each word with a long list of numbers.

00:04:57.860 --> 00:05:02.396
The reason for this is that the training process only works with continuous values,

00:05:02.396 --> 00:05:05.312
so you have to somehow encode language using numbers,

00:05:05.312 --> 00:05:09.254
and each of these lists of numbers may somehow encode the meaning of the

00:05:09.254 --> 00:05:10.280
corresponding word.

00:05:10.280 --> 00:05:13.360
What makes transformers unique is their reliance

00:05:13.360 --> 00:05:16.000
on a special operation known as attention.

00:05:16.980 --> 00:05:21.684
This operation gives all of these lists of numbers a chance to talk to one another

00:05:21.684 --> 00:05:26.560
and refine the meanings they encode based on the context around, all done in parallel.

00:05:27.400 --> 00:05:31.707
For example, the numbers encoding the word bank might be changed based on the

00:05:31.707 --> 00:05:36.180
context surrounding it to somehow encode the more specific notion of a riverbank.

00:05:37.280 --> 00:05:41.029
Transformers typically also include a second type of operation known

00:05:41.029 --> 00:05:44.561
as a feed-forward neural network, and this gives the model extra

00:05:44.561 --> 00:05:48.420
capacity to store more patterns about language learned during training.

00:05:49.280 --> 00:05:53.401
All of this data repeatedly flows through many different iterations of

00:05:53.401 --> 00:05:56.478
these two fundamental operations, and as it does so,

00:05:56.478 --> 00:06:00.484
the hope is that each list of numbers is enriched to encode whatever

00:06:00.484 --> 00:06:04.664
information might be needed to make an accurate prediction of what word

00:06:04.664 --> 00:06:06.000
follows in the passage.

00:06:07.000 --> 00:06:11.534
At the end, one final function is performed on the last vector in this sequence,

00:06:11.534 --> 00:06:16.573
which now has had a chance to be influenced by all the other context from the input text,

00:06:16.573 --> 00:06:19.764
as well as everything the model learned during training,

00:06:19.764 --> 00:06:22.060
to produce a prediction of the next word.

00:06:22.480 --> 00:06:27.360
Again, the model's prediction looks like a probability for every possible next word.

00:06:28.560 --> 00:06:32.794
Although researchers design the framework for how each of these steps work,

00:06:32.794 --> 00:06:37.362
it's important to understand that the specific behavior is an emergent phenomenon

00:06:37.362 --> 00:06:41.820
based on how those hundreds of billions of parameters are tuned during training.

00:06:42.480 --> 00:06:45.070
This makes it incredibly challenging to determine

00:06:45.070 --> 00:06:47.920
why the model makes the exact predictions that it does.

00:06:48.440 --> 00:06:53.778
What you can see is that when you use large language model predictions to autocomplete

00:06:53.778 --> 00:06:59.240
a prompt, the words that it generates are uncannily fluent, fascinating, and even useful.

00:07:05.719 --> 00:07:08.826
If you're a new viewer and you're curious about more details on how

00:07:08.826 --> 00:07:11.979
transformers and attention work, boy do I have some material for you.

00:07:12.399 --> 00:07:16.080
One option is to jump into a series I made about deep learning,

00:07:16.080 --> 00:07:20.740
where we visualize and motivate the details of attention and all the other steps

00:07:20.740 --> 00:07:21.719
in a transformer.

00:07:22.099 --> 00:07:25.529
Also, on my second channel I just posted a talk I gave a couple

00:07:25.529 --> 00:07:28.639
months ago about this topic for the company TNG in Munich.

00:07:29.079 --> 00:07:33.101
Sometimes I actually prefer the content I make as a casual talk rather than a produced

00:07:33.101 --> 00:07:36.939
video, but I leave it up to you which one of these feels like the better follow-on.

