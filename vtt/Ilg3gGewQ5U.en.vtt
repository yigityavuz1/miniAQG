WEBVTT
Kind: captions
Language: en

00:00:04.060 --> 00:00:08.880
Here, we tackle backpropagation, the core algorithm behind how neural networks learn.

00:00:09.400 --> 00:00:13.379
After a quick recap for where we are, the first thing I'll do is an intuitive walkthrough

00:00:13.379 --> 00:00:17.000
for what the algorithm is actually doing, without any reference to the formulas.

00:00:17.660 --> 00:00:20.294
Then, for those of you who do want to dive into the math,

00:00:20.294 --> 00:00:23.020
the next video goes into the calculus underlying all this.

00:00:23.820 --> 00:00:27.410
If you watched the last two videos, or if you're just jumping in with the appropriate

00:00:27.410 --> 00:00:31.000
background, you know what a neural network is, and how it feeds forward information.

00:00:31.680 --> 00:00:36.007
Here, we're doing the classic example of recognizing handwritten digits whose pixel

00:00:36.007 --> 00:00:39.604
values get fed into the first layer of the network with 784 neurons,

00:00:39.604 --> 00:00:43.983
and I've been showing a network with two hidden layers having just 16 neurons each,

00:00:43.983 --> 00:00:48.258
and an output layer of 10 neurons, indicating which digit the network is choosing

00:00:48.258 --> 00:00:49.040
as its answer.

00:00:50.040 --> 00:00:53.039
I'm also expecting you to understand gradient descent,

00:00:53.039 --> 00:00:56.816
as described in the last video, and how what we mean by learning is

00:00:56.816 --> 00:01:01.260
that we want to find which weights and biases minimize a certain cost function.

00:01:02.040 --> 00:01:05.755
As a quick reminder, for the cost of a single training example,

00:01:05.755 --> 00:01:10.708
you take the output the network gives, along with the output you wanted it to give,

00:01:10.708 --> 00:01:14.600
and add up the squares of the differences between each component.

00:01:15.380 --> 00:01:18.790
Doing this for all of your tens of thousands of training examples and

00:01:18.790 --> 00:01:22.200
averaging the results, this gives you the total cost of the network.

00:01:22.200 --> 00:01:26.271
And as if that's not enough to think about, as described in the last video,

00:01:26.271 --> 00:01:30.667
the thing that we're looking for is the negative gradient of this cost function,

00:01:30.667 --> 00:01:34.466
which tells you how you need to change all of the weights and biases,

00:01:34.466 --> 00:01:38.320
all of these connections, so as to most efficiently decrease the cost.

00:01:43.260 --> 00:01:45.664
Backpropagation, the topic of this video, is an

00:01:45.664 --> 00:01:48.580
algorithm for computing that crazy complicated gradient.

00:01:49.140 --> 00:01:52.876
And the one idea from the last video that I really want you to hold firmly

00:01:52.876 --> 00:01:56.461
in your mind right now is that because thinking of the gradient vector

00:01:56.461 --> 00:01:59.440
as a direction in 13,000 dimensions is, to put it lightly,

00:01:59.440 --> 00:02:03.580
beyond the scope of our imaginations, there's another way you can think about it.

00:02:04.600 --> 00:02:07.741
The magnitude of each component here is telling you how

00:02:07.741 --> 00:02:10.940
sensitive the cost function is to each weight and bias.

00:02:11.800 --> 00:02:15.744
For example, let's say you go through the process I'm about to describe,

00:02:15.744 --> 00:02:20.509
and you compute the negative gradient, and the component associated with the weight on

00:02:20.509 --> 00:02:25.274
this edge here comes out to be 3.2, while the component associated with this edge here

00:02:25.274 --> 00:02:26.260
comes out as 0.1.

00:02:26.820 --> 00:02:30.969
The way you would interpret that is that the cost of the function is 32 times more

00:02:30.969 --> 00:02:34.965
sensitive to changes in that first weight, so if you were to wiggle that value

00:02:34.965 --> 00:02:38.203
just a little bit, it's going to cause some change to the cost,

00:02:38.203 --> 00:02:42.099
and that change is 32 times greater than what the same wiggle to that second

00:02:42.099 --> 00:02:43.060
weight would give.

00:02:48.420 --> 00:02:51.368
Personally, when I was first learning about backpropagation,

00:02:51.368 --> 00:02:55.740
I think the most confusing aspect was just the notation and the index chasing of it all.

00:02:56.220 --> 00:02:59.444
But once you unwrap what each part of this algorithm is really doing,

00:02:59.444 --> 00:03:02.481
each individual effect it's having is actually pretty intuitive,

00:03:02.481 --> 00:03:06.640
it's just that there's a lot of little adjustments getting layered on top of each other.

00:03:07.740 --> 00:03:11.780
So I'm going to start things off here with a complete disregard for the notation,

00:03:11.780 --> 00:03:16.120
and just step through the effects each training example has on the weights and biases.

00:03:17.020 --> 00:03:21.733
Because the cost function involves averaging a certain cost per example over all

00:03:21.733 --> 00:03:26.327
the tens of thousands of training examples, the way we adjust the weights and

00:03:26.327 --> 00:03:31.040
biases for a single gradient descent step also depends on every single example.

00:03:31.680 --> 00:03:35.531
Or rather, in principle it should, but for computational efficiency we'll do a little

00:03:35.531 --> 00:03:39.200
trick later to keep you from needing to hit every single example for every step.

00:03:39.200 --> 00:03:42.610
In other cases, right now, all we're going to do is focus

00:03:42.610 --> 00:03:45.960
our attention on one single example, this image of a 2.

00:03:46.720 --> 00:03:49.228
What effect should this one training example have

00:03:49.228 --> 00:03:51.480
on how the weights and biases get adjusted?

00:03:52.680 --> 00:03:56.213
Let's say we're at a point where the network is not well trained yet,

00:03:56.213 --> 00:03:59.593
so the activations in the output are going to look pretty random,

00:03:59.593 --> 00:04:02.000
maybe something like 0.5, 0.8, 0.2, on and on.

00:04:02.520 --> 00:04:04.815
We can't directly change those activations, we

00:04:04.815 --> 00:04:07.160
only have influence on the weights and biases.

00:04:07.160 --> 00:04:09.952
But it's helpful to keep track of which adjustments

00:04:09.952 --> 00:04:12.580
we wish should take place to that output layer.

00:04:13.360 --> 00:04:16.398
And since we want it to classify the image as a 2,

00:04:16.398 --> 00:04:21.260
we want that third value to get nudged up while all the others get nudged down.

00:04:22.060 --> 00:04:25.696
Moreover, the sizes of these nudges should be proportional

00:04:25.696 --> 00:04:29.520
to how far away each current value is from its target value.

00:04:30.220 --> 00:04:33.780
For example, the increase to that number 2 neuron's activation

00:04:33.780 --> 00:04:37.857
is in a sense more important than the decrease to the number 8 neuron,

00:04:37.857 --> 00:04:40.900
which is already pretty close to where it should be.

00:04:42.040 --> 00:04:44.984
So zooming in further, let's focus just on this one neuron,

00:04:44.984 --> 00:04:47.280
the one whose activation we wish to increase.

00:04:48.180 --> 00:04:52.526
Remember, that activation is defined as a certain weighted sum of all the

00:04:52.526 --> 00:04:55.384
activations in the previous layer, plus a bias,

00:04:55.384 --> 00:05:00.385
which is all then plugged into something like the sigmoid squishification function,

00:05:00.385 --> 00:05:01.040
or a ReLU.

00:05:01.640 --> 00:05:04.442
So there are three different avenues that can team

00:05:04.442 --> 00:05:07.020
up together to help increase that activation.

00:05:07.440 --> 00:05:10.626
You can increase the bias, you can increase the weights,

00:05:10.626 --> 00:05:14.040
and you can change the activations from the previous layer.

00:05:14.940 --> 00:05:17.359
Focusing on how the weights should be adjusted,

00:05:17.359 --> 00:05:20.860
notice how the weights actually have differing levels of influence.

00:05:21.440 --> 00:05:25.196
The connections with the brightest neurons from the preceding layer have the

00:05:25.196 --> 00:05:29.100
biggest effect since those weights are multiplied by larger activation values.

00:05:31.460 --> 00:05:33.884
So if you were to increase one of those weights,

00:05:33.884 --> 00:05:38.076
it actually has a stronger influence on the ultimate cost function than increasing

00:05:38.076 --> 00:05:40.500
the weights of connections with dimmer neurons,

00:05:40.500 --> 00:05:43.480
at least as far as this one training example is concerned.

00:05:44.420 --> 00:05:46.585
Remember, when we talk about gradient descent,

00:05:46.585 --> 00:05:50.255
we don't just care about whether each component should get nudged up or down,

00:05:50.255 --> 00:05:53.220
we care about which ones give you the most bang for your buck.

00:05:55.020 --> 00:05:58.511
This, by the way, is at least somewhat reminiscent of a theory in

00:05:58.511 --> 00:06:02.539
neuroscience for how biological networks of neurons learn, Hebbian theory,

00:06:02.539 --> 00:06:06.460
often summed up in the phrase, neurons that fire together wire together.

00:06:07.260 --> 00:06:11.719
Here, the biggest increases to weights, the biggest strengthening of connections,

00:06:11.719 --> 00:06:14.527
happens between neurons which are the most active,

00:06:14.527 --> 00:06:17.280
and the ones which we wish to become more active.

00:06:17.940 --> 00:06:21.156
In a sense, the neurons that are firing while seeing a 2 get

00:06:21.156 --> 00:06:24.480
more strongly linked to those firing when thinking about a 2.

00:06:25.400 --> 00:06:29.356
To be clear, I'm not in a position to make statements one way or another about

00:06:29.356 --> 00:06:33.362
whether artificial networks of neurons behave anything like biological brains,

00:06:33.362 --> 00:06:37.673
and this fires together wire together idea comes with a couple meaningful asterisks,

00:06:37.673 --> 00:06:41.020
but taken as a very loose analogy, I find it interesting to note.

00:06:41.940 --> 00:06:45.746
Anyway, the third way we can help increase this neuron's activation

00:06:45.746 --> 00:06:49.040
is by changing all the activations in the previous layer.

00:06:49.040 --> 00:06:53.015
Namely, if everything connected to that digit 2 neuron with a positive

00:06:53.015 --> 00:06:57.784
weight got brighter, and if everything connected with a negative weight got dimmer,

00:06:57.784 --> 00:07:00.680
then that digit 2 neuron would become more active.

00:07:02.540 --> 00:07:06.387
And similar to the weight changes, you're going to get the most bang for your buck

00:07:06.387 --> 00:07:10.280
by seeking changes that are proportional to the size of the corresponding weights.

00:07:12.140 --> 00:07:15.096
Now of course, we cannot directly influence those activations,

00:07:15.096 --> 00:07:17.480
we only have control over the weights and biases.

00:07:17.480 --> 00:07:20.835
But just as with the last layer, it's helpful to

00:07:20.835 --> 00:07:24.120
keep a note of what those desired changes are.

00:07:24.580 --> 00:07:26.938
But keep in mind, zooming out one step here, this

00:07:26.938 --> 00:07:29.200
is only what that digit 2 output neuron wants.

00:07:29.760 --> 00:07:33.942
Remember, we also want all the other neurons in the last layer to become less active,

00:07:33.942 --> 00:07:37.189
and each of those other output neurons has its own thoughts about

00:07:37.189 --> 00:07:39.600
what should happen to that second to last layer.

00:07:42.700 --> 00:07:47.401
So, the desire of this digit 2 neuron is added together with the desires

00:07:47.401 --> 00:07:52.951
of all the other output neurons for what should happen to this second to last layer,

00:07:52.951 --> 00:07:56.215
again in proportion to the corresponding weights,

00:07:56.215 --> 00:08:00.720
and in proportion to how much each of those neurons needs to change.

00:08:01.600 --> 00:08:05.480
This right here is where the idea of propagating backwards comes in.

00:08:05.820 --> 00:08:09.477
By adding together all these desired effects, you basically get a

00:08:09.477 --> 00:08:13.360
list of nudges that you want to happen to this second to last layer.

00:08:14.220 --> 00:08:17.847
And once you have those, you can recursively apply the same process to the

00:08:17.847 --> 00:08:20.640
relevant weights and biases that determine those values,

00:08:20.640 --> 00:08:24.071
repeating the same process I just walked through and moving backwards

00:08:24.071 --> 00:08:25.100
through the network.

00:08:28.960 --> 00:08:33.063
And zooming out a bit further, remember that this is all just how a single

00:08:33.063 --> 00:08:37.000
training example wishes to nudge each one of those weights and biases.

00:08:37.480 --> 00:08:40.280
If we only listened to what that 2 wanted, the network would

00:08:40.280 --> 00:08:43.220
ultimately be incentivized just to classify all images as a 2.

00:08:44.060 --> 00:08:49.244
So what you do is go through this same backprop routine for every other training example,

00:08:49.244 --> 00:08:53.437
recording how each of them would like to change the weights and biases,

00:08:53.437 --> 00:08:56.000
and average together those desired changes.

00:09:01.720 --> 00:09:05.883
This collection here of the averaged nudges to each weight and bias is,

00:09:05.883 --> 00:09:10.104
loosely speaking, the negative gradient of the cost function referenced

00:09:10.104 --> 00:09:13.680
in the last video, or at least something proportional to it.

00:09:14.380 --> 00:09:18.390
I say loosely speaking only because I have yet to get quantitatively precise

00:09:18.390 --> 00:09:22.294
about those nudges, but if you understood every change I just referenced,

00:09:22.294 --> 00:09:24.827
why some are proportionally bigger than others,

00:09:24.827 --> 00:09:28.890
and how they all need to be added together, you understand the mechanics for

00:09:28.890 --> 00:09:31.000
what backpropagation is actually doing.

00:09:33.960 --> 00:09:38.229
By the way, in practice, it takes computers an extremely long time to add

00:09:38.229 --> 00:09:42.440
up the influence of every training example every gradient descent step.

00:09:43.140 --> 00:09:44.820
So here's what's commonly done instead.

00:09:45.480 --> 00:09:48.926
You randomly shuffle your training data and then divide it into a whole

00:09:48.926 --> 00:09:52.420
bunch of mini-batches, let's say each one having 100 training examples.

00:09:52.940 --> 00:09:56.200
Then you compute a step according to the mini-batch.

00:09:56.960 --> 00:10:00.012
It's not going to be the actual gradient of the cost function,

00:10:00.012 --> 00:10:03.211
which depends on all of the training data, not this tiny subset,

00:10:03.211 --> 00:10:05.475
so it's not the most efficient step downhill,

00:10:05.475 --> 00:10:09.659
but each mini-batch does give you a pretty good approximation, and more importantly,

00:10:09.659 --> 00:10:12.120
it gives you a significant computational speedup.

00:10:12.820 --> 00:10:17.078
If you were to plot the trajectory of your network under the relevant cost surface,

00:10:17.078 --> 00:10:21.490
it would be a little more like a drunk man stumbling aimlessly down a hill but taking

00:10:21.490 --> 00:10:25.799
quick steps, rather than a carefully calculating man determining the exact downhill

00:10:25.799 --> 00:10:30.160
direction of each step before taking a very slow and careful step in that direction.

00:10:31.540 --> 00:10:34.660
This technique is referred to as stochastic gradient descent.

00:10:35.960 --> 00:10:39.620
There's a lot going on here, so let's just sum it up for ourselves, shall we?

00:10:40.440 --> 00:10:44.220
Backpropagation is the algorithm for determining how a single training

00:10:44.220 --> 00:10:47.028
example would like to nudge the weights and biases,

00:10:47.028 --> 00:10:50.052
not just in terms of whether they should go up or down,

00:10:50.052 --> 00:10:53.778
but in terms of what relative proportions to those changes cause the

00:10:53.778 --> 00:10:55.560
most rapid decrease to the cost.

00:10:56.260 --> 00:11:00.230
A true gradient descent step would involve doing this for all your tens of

00:11:00.230 --> 00:11:04.200
thousands of training examples and averaging the desired changes you get.

00:11:04.860 --> 00:11:08.906
But that's computationally slow, so instead you randomly subdivide the

00:11:08.906 --> 00:11:13.240
data into mini-batches and compute each step with respect to a mini-batch.

00:11:14.000 --> 00:11:17.863
Repeatedly going through all of the mini-batches and making these adjustments,

00:11:17.863 --> 00:11:21.033
you will converge towards a local minimum of the cost function,

00:11:21.033 --> 00:11:25.045
which is to say your network will end up doing a really good job on the training

00:11:25.045 --> 00:11:25.540
examples.

00:11:27.240 --> 00:11:32.036
So with all of that said, every line of code that would go into implementing backprop

00:11:32.036 --> 00:11:36.720
actually corresponds with something you have now seen, at least in informal terms.

00:11:37.560 --> 00:11:40.481
But sometimes knowing what the math does is only half the battle,

00:11:40.481 --> 00:11:44.120
and just representing the damn thing is where it gets all muddled and confusing.

00:11:44.860 --> 00:11:48.577
So for those of you who do want to go deeper, the next video goes through the same

00:11:48.577 --> 00:11:52.113
ideas that were just presented here, but in terms of the underlying calculus,

00:11:52.113 --> 00:11:55.921
which should hopefully make it a little more familiar as you see the topic in other

00:11:55.921 --> 00:11:56.420
resources.

00:11:57.340 --> 00:12:00.838
Before that, one thing worth emphasizing is that for this algorithm to work,

00:12:00.838 --> 00:12:04.381
and this goes for all sorts of machine learning beyond just neural networks,

00:12:04.381 --> 00:12:05.900
you need a lot of training data.

00:12:06.420 --> 00:12:10.654
In our case, one thing that makes handwritten digits such a nice example is that there

00:12:10.654 --> 00:12:14.740
exists the MNIST database, with so many examples that have been labeled by humans.

00:12:15.300 --> 00:12:19.204
So a common challenge that those of you working in machine learning will be familiar with

00:12:19.204 --> 00:12:21.880
is just getting the labeled training data you actually need,

00:12:21.880 --> 00:12:24.687
whether that's having people label tens of thousands of images,

00:12:24.687 --> 00:12:27.100
or whatever other data type you might be dealing with.

