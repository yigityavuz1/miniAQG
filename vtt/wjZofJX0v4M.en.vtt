WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.560
The initials GPT stand for Generative Pretrained Transformer.

00:00:05.220 --> 00:00:09.020
So that first word is straightforward enough, these are bots that generate new text.

00:00:09.800 --> 00:00:13.181
Pretrained refers to how the model went through a process of learning

00:00:13.181 --> 00:00:16.610
from a massive amount of data, and the prefix insinuates that there's

00:00:16.610 --> 00:00:20.040
more room to fine-tune it on specific tasks with additional training.

00:00:20.720 --> 00:00:22.900
But the last word, that's the real key piece.

00:00:23.380 --> 00:00:27.571
A transformer is a specific kind of neural network, a machine learning model,

00:00:27.571 --> 00:00:31.000
and it's the core invention underlying the current boom in AI.

00:00:31.740 --> 00:00:35.430
What I want to do with this video and the following chapters is go through a

00:00:35.430 --> 00:00:39.120
visually-driven explanation for what actually happens inside a transformer.

00:00:39.700 --> 00:00:42.820
We're going to follow the data that flows through it and go step by step.

00:00:43.440 --> 00:00:47.380
There are many different kinds of models that you can build using transformers.

00:00:47.800 --> 00:00:50.800
Some models take in audio and produce a transcript.

00:00:51.340 --> 00:00:54.183
This sentence comes from a model going the other way around,

00:00:54.183 --> 00:00:56.220
producing synthetic speech just from text.

00:00:56.660 --> 00:01:01.062
All those tools that took the world by storm in 2022 like DALL-E and Midjourney

00:01:01.062 --> 00:01:05.520
that take in a text description and produce an image are based on transformers.

00:01:06.000 --> 00:01:09.737
Even if I can't quite get it to understand what a pi creature is supposed to be,

00:01:09.737 --> 00:01:13.100
I'm still blown away that this kind of thing is even remotely possible.

00:01:13.900 --> 00:01:18.000
And the original transformer introduced in 2017 by Google was invented for

00:01:18.000 --> 00:01:22.100
the specific use case of translating text from one language into another.

00:01:22.660 --> 00:01:26.395
But the variant that you and I will focus on, which is the type that

00:01:26.395 --> 00:01:31.284
underlies tools like ChatGPT, will be a model that's trained to take in a piece of text,

00:01:31.284 --> 00:01:34.909
maybe even with some surrounding images or sound accompanying it,

00:01:34.909 --> 00:01:38.260
and produce a prediction for what comes next in the passage.

00:01:38.600 --> 00:01:41.337
That prediction takes the form of a probability distribution

00:01:41.337 --> 00:01:43.800
over many different chunks of text that might follow.

00:01:45.040 --> 00:01:47.551
At first glance, you might think that predicting the next word

00:01:47.551 --> 00:01:49.940
feels like a very different goal from generating new text.

00:01:50.180 --> 00:01:52.519
But once you have a prediction model like this,

00:01:52.519 --> 00:01:56.202
a simple thing you could try to make it generate, a longer piece of text,

00:01:56.202 --> 00:01:58.541
is to give it an initial snippet to work with,

00:01:58.541 --> 00:02:02.025
have it take a random sample from the distribution it just generated,

00:02:02.025 --> 00:02:05.857
append that sample to the text, and then run the whole process again to make

00:02:05.857 --> 00:02:09.540
a new prediction based on all the new text, including what it just added.

00:02:10.100 --> 00:02:13.000
I don't know about you, but it really doesn't feel like this should actually work.

00:02:13.420 --> 00:02:17.946
In this animation, for example, I'm running GPT-2 on my laptop and having it repeatedly

00:02:17.946 --> 00:02:22.420
predict and sample the next chunk of text to generate a story based on the seed text.

00:02:22.420 --> 00:02:26.120
The story just doesn't actually really make that much sense.

00:02:26.500 --> 00:02:31.293
But if I swap it out for API calls to GPT-3 instead, which is the same basic model,

00:02:31.293 --> 00:02:35.451
just much bigger, suddenly almost magically we do get a sensible story,

00:02:35.451 --> 00:02:40.129
one that even seems to infer that a pi creature would live in a land of math and

00:02:40.129 --> 00:02:40.880
computation.

00:02:41.580 --> 00:02:44.931
This process here of repeated prediction and sampling is essentially

00:02:44.931 --> 00:02:47.346
what's happening when you interact with ChatGPT,

00:02:47.346 --> 00:02:50.894
or any of these other large language models, and you see them producing

00:02:50.894 --> 00:02:51.880
one word at a time.

00:02:52.480 --> 00:02:55.850
In fact, one feature that I would very much enjoy is the ability to

00:02:55.850 --> 00:02:59.220
see the underlying distribution for each new word that it chooses.

00:03:03.820 --> 00:03:06.258
Let's kick things off with a very high level preview

00:03:06.258 --> 00:03:08.180
of how data flows through a transformer.

00:03:08.640 --> 00:03:11.964
We will spend much more time motivating and interpreting and expanding

00:03:11.964 --> 00:03:14.386
on the details of each step, but in broad strokes,

00:03:14.386 --> 00:03:18.660
when one of these chatbots generates a given word, here's what's going on under the hood.

00:03:19.080 --> 00:03:22.040
First, the input is broken up into a bunch of little pieces.

00:03:22.620 --> 00:03:26.220
These pieces are called tokens, and in the case of text these tend to be

00:03:26.220 --> 00:03:29.820
words or little pieces of words or other common character combinations.

00:03:30.740 --> 00:03:34.077
If images or sound are involved, then tokens could be little

00:03:34.077 --> 00:03:37.080
patches of that image or little chunks of that sound.

00:03:37.580 --> 00:03:42.206
Each one of these tokens is then associated with a vector, meaning some list of numbers,

00:03:42.206 --> 00:03:45.360
which is meant to somehow encode the meaning of that piece.

00:03:45.880 --> 00:03:50.089
If you think of these vectors as giving coordinates in some very high dimensional space,

00:03:50.089 --> 00:03:53.006
words with similar meanings tend to land on vectors that are

00:03:53.006 --> 00:03:54.680
close to each other in that space.

00:03:55.280 --> 00:03:58.187
This sequence of vectors then passes through an operation that's

00:03:58.187 --> 00:04:01.275
known as an attention block, and this allows the vectors to talk to

00:04:01.275 --> 00:04:04.500
each other and pass information back and forth to update their values.

00:04:04.880 --> 00:04:08.482
For example, the meaning of the word model in the phrase "a machine learning

00:04:08.482 --> 00:04:11.800
model" is different from its meaning in the phrase "a fashion model".

00:04:12.260 --> 00:04:15.510
The attention block is what's responsible for figuring out which

00:04:15.510 --> 00:04:19.421
words in context are relevant to updating the meanings of which other words,

00:04:19.421 --> 00:04:21.960
and how exactly those meanings should be updated.

00:04:22.500 --> 00:04:25.092
And again, whenever I use the word meaning, this is

00:04:25.092 --> 00:04:28.040
somehow entirely encoded in the entries of those vectors.

00:04:29.180 --> 00:04:32.276
After that, these vectors pass through a different kind of operation,

00:04:32.276 --> 00:04:35.418
and depending on the source that you're reading this will be referred

00:04:35.418 --> 00:04:38.200
to as a multi-layer perceptron or maybe a feed-forward layer.

00:04:38.580 --> 00:04:40.495
And here the vectors don't talk to each other,

00:04:40.495 --> 00:04:42.660
they all go through the same operation in parallel.

00:04:43.060 --> 00:04:45.748
And while this block is a little bit harder to interpret,

00:04:45.748 --> 00:04:49.473
later on we'll talk about how the step is a little bit like asking a long list

00:04:49.473 --> 00:04:53.057
of questions about each vector, and then updating them based on the answers

00:04:53.057 --> 00:04:54.000
to those questions.

00:04:54.900 --> 00:04:58.181
All of the operations in both of these blocks look like a

00:04:58.181 --> 00:05:01.693
giant pile of matrix multiplications, and our primary job is

00:05:01.693 --> 00:05:05.320
going to be to understand how to read the underlying matrices.

00:05:06.980 --> 00:05:10.933
I'm glossing over some details about some normalization steps that happen in between,

00:05:10.933 --> 00:05:12.980
but this is after all a high-level preview.

00:05:13.680 --> 00:05:17.237
After that, the process essentially repeats, you go back and forth

00:05:17.237 --> 00:05:20.470
between attention blocks and multi-layer perceptron blocks,

00:05:20.470 --> 00:05:24.135
until at the very end the hope is that all of the essential meaning

00:05:24.135 --> 00:05:28.500
of the passage has somehow been baked into the very last vector in the sequence.

00:05:28.920 --> 00:05:33.325
We then perform a certain operation on that last vector that produces a probability

00:05:33.325 --> 00:05:37.836
distribution over all possible tokens, all possible little chunks of text that might

00:05:37.836 --> 00:05:38.420
come next.

00:05:38.980 --> 00:05:42.318
And like I said, once you have a tool that predicts what comes next

00:05:42.318 --> 00:05:45.856
given a snippet of text, you can feed it a little bit of seed text and

00:05:45.856 --> 00:05:49.094
have it repeatedly play this game of predicting what comes next,

00:05:49.094 --> 00:05:53.080
sampling from the distribution, appending it, and then repeating over and over.

00:05:53.640 --> 00:05:57.944
Some of you in the know may remember how long before ChatGPT came into the scene,

00:05:57.944 --> 00:06:00.442
this is what early demos of GPT-3 looked like,

00:06:00.442 --> 00:06:04.640
you would have it autocomplete stories and essays based on an initial snippet.

00:06:05.580 --> 00:06:09.820
To make a tool like this into a chatbot, the easiest starting point is to have a

00:06:09.820 --> 00:06:13.901
little bit of text that establishes the setting of a user interacting with a

00:06:13.901 --> 00:06:17.135
helpful AI assistant, what you would call the system prompt,

00:06:17.135 --> 00:06:21.428
and then you would use the user's initial question or prompt as the first bit of

00:06:21.428 --> 00:06:25.721
dialogue, and then you have it start predicting what such a helpful AI assistant

00:06:25.721 --> 00:06:26.940
would say in response.

00:06:27.720 --> 00:06:30.974
There is more to say about an added step of training that's required

00:06:30.974 --> 00:06:33.940
to make this work well, but at a high level this is the idea.

00:06:35.720 --> 00:06:39.964
In this chapter, you and I are going to expand on the details of what happens at the very

00:06:39.964 --> 00:06:42.729
beginning of the network, at the very end of the network,

00:06:42.729 --> 00:06:46.687
and I also want to spend a lot of time reviewing some important bits of background

00:06:46.687 --> 00:06:50.931
knowledge, things that would have been second nature to any machine learning engineer by

00:06:50.931 --> 00:06:52.600
the time transformers came around.

00:06:53.060 --> 00:06:56.217
If you're comfortable with that background knowledge and a little impatient,

00:06:56.217 --> 00:06:58.626
you could probably feel free to skip to the next chapter,

00:06:58.626 --> 00:07:00.662
which is going to focus on the attention blocks,

00:07:00.662 --> 00:07:02.780
generally considered the heart of the transformer.

00:07:03.360 --> 00:07:06.982
After that, I want to talk more about these multi-layer perceptron blocks,

00:07:06.982 --> 00:07:11.093
how training works, and a number of other details that will have been skipped up to

00:07:11.093 --> 00:07:11.680
that point.

00:07:12.180 --> 00:07:16.206
For broader context, these videos are additions to a mini-series about deep learning,

00:07:16.206 --> 00:07:18.858
and it's okay if you haven't watched the previous ones,

00:07:18.858 --> 00:07:22.931
I think you can do it out of order, but before diving into transformers specifically,

00:07:22.931 --> 00:07:27.004
I do think it's worth making sure that we're on the same page about the basic premise

00:07:27.004 --> 00:07:28.520
and structure of deep learning.

00:07:29.020 --> 00:07:33.248
At the risk of stating the obvious, this is one approach to machine learning,

00:07:33.248 --> 00:07:37.806
which describes any model where you're using data to somehow determine how a model

00:07:37.806 --> 00:07:38.300
behaves.

00:07:39.140 --> 00:07:42.475
What I mean by that is, let's say you want a function that takes in

00:07:42.475 --> 00:07:44.865
an image and it produces a label describing it,

00:07:44.865 --> 00:07:48.250
or our example of predicting the next word given a passage of text,

00:07:48.250 --> 00:07:51.535
or any other task that seems to require some element of intuition

00:07:51.535 --> 00:07:52.780
and pattern recognition.

00:07:53.200 --> 00:07:57.695
We almost take this for granted these days, but the idea with machine learning is that

00:07:57.695 --> 00:08:02.138
rather than trying to explicitly define a procedure for how to do that task in code,

00:08:02.138 --> 00:08:05.535
which is what people would have done in the earliest days of AI,

00:08:05.535 --> 00:08:09.194
instead you set up a very flexible structure with tunable parameters,

00:08:09.194 --> 00:08:11.912
like a bunch of knobs and dials, and then, somehow,

00:08:11.912 --> 00:08:16.355
you use many examples of what the output should look like for a given input to tweak

00:08:16.355 --> 00:08:19.700
and tune the values of those parameters to mimic this behavior.

00:08:19.700 --> 00:08:24.117
For example, maybe the simplest form of machine learning is linear regression,

00:08:24.117 --> 00:08:27.231
where your inputs and outputs are each single numbers,

00:08:27.231 --> 00:08:30.628
something like the square footage of a house and its price,

00:08:30.628 --> 00:08:34.988
and what you want is to find a line of best fit through this data, you know,

00:08:34.988 --> 00:08:36.800
to predict future house prices.

00:08:37.440 --> 00:08:40.520
That line is described by two continuous parameters,

00:08:40.520 --> 00:08:43.955
say the slope and the y-intercept, and the goal of linear

00:08:43.955 --> 00:08:48.160
regression is to determine those parameters to closely match the data.

00:08:48.880 --> 00:08:52.100
Needless to say, deep learning models get much more complicated.

00:08:52.620 --> 00:08:57.660
GPT-3, for example, has not two, but 175 billion parameters.

00:08:58.120 --> 00:09:01.952
But here's the thing, it's not a given that you can create some giant

00:09:01.952 --> 00:09:05.562
model with a huge number of parameters without it either grossly

00:09:05.562 --> 00:09:09.560
overfitting the training data or being completely intractable to train.

00:09:10.260 --> 00:09:13.087
Deep learning describes a class of models that in the

00:09:13.087 --> 00:09:16.180
last couple decades have proven to scale remarkably well.

00:09:16.480 --> 00:09:19.658
What unifies them is that they all use the same training algorithm,

00:09:19.658 --> 00:09:22.979
it's called backpropagation, we talked about it in previous chapters,

00:09:22.979 --> 00:09:26.679
and the context that I want you to have as we go in is that in order for this

00:09:26.679 --> 00:09:30.474
training algorithm to work well at scale, these models have to follow a certain

00:09:30.474 --> 00:09:31.280
specific format.

00:09:31.800 --> 00:09:36.051
And if you know this format going in, it helps to explain many of the choices for how a

00:09:36.051 --> 00:09:40.400
transformer processes language, which otherwise run the risk of feeling kinda arbitrary.

00:09:41.440 --> 00:09:43.910
First, whatever kind of model you're making, the

00:09:43.910 --> 00:09:46.740
input has to be formatted as an array of real numbers.

00:09:46.740 --> 00:09:50.939
This could simply mean a list of numbers, it could be a two-dimensional array,

00:09:50.939 --> 00:09:53.900
or very often you deal with higher dimensional arrays,

00:09:53.900 --> 00:09:56.000
where the general term used is tensor.

00:09:56.560 --> 00:10:00.517
You often think of that input data as being progressively transformed into many

00:10:00.517 --> 00:10:04.423
distinct layers, where again, each layer is always structured as some kind of

00:10:04.423 --> 00:10:08.680
array of real numbers, until you get to a final layer which you consider the output.

00:10:09.280 --> 00:10:13.326
For example, the final layer in our text processing model is a list of numbers

00:10:13.326 --> 00:10:17.060
representing the probability distribution for all possible next tokens.

00:10:17.820 --> 00:10:22.035
In deep learning, these model parameters are almost always referred to as weights,

00:10:22.035 --> 00:10:25.993
and this is because a key feature of these models is that the only way these

00:10:25.993 --> 00:10:29.900
parameters interact with the data being processed is through weighted sums.

00:10:30.340 --> 00:10:32.743
You also sprinkle some non-linear functions throughout,

00:10:32.743 --> 00:10:34.360
but they won't depend on parameters.

00:10:35.200 --> 00:10:38.620
Typically, though, instead of seeing the weighted sums all naked

00:10:38.620 --> 00:10:41.986
and written out explicitly like this, you'll instead find them

00:10:41.986 --> 00:10:45.620
packaged together as various components in a matrix vector product.

00:10:46.740 --> 00:10:50.416
It amounts to saying the same thing, if you think back to how matrix vector

00:10:50.416 --> 00:10:54.240
multiplication works, each component in the output looks like a weighted sum.

00:10:54.780 --> 00:10:58.250
It's just often conceptually cleaner for you and me to think

00:10:58.250 --> 00:11:01.719
about matrices that are filled with tunable parameters that

00:11:01.719 --> 00:11:05.420
transform vectors that are drawn from the data being processed.

00:11:06.340 --> 00:11:10.212
For example, those 175 billion weights in GPT-3 are

00:11:10.212 --> 00:11:14.160
organized into just under 28,000 distinct matrices.

00:11:14.660 --> 00:11:17.417
Those matrices in turn fall into eight different categories,

00:11:17.417 --> 00:11:21.230
and what you and I are going to do is step through each one of those categories to

00:11:21.230 --> 00:11:22.700
understand what that type does.

00:11:23.160 --> 00:11:27.087
As we go through, I think it's kind of fun to reference the specific

00:11:27.087 --> 00:11:31.360
numbers from GPT-3 to count up exactly where those 175 billion come from.

00:11:31.880 --> 00:11:34.411
Even if nowadays there are bigger and better models,

00:11:34.411 --> 00:11:37.186
this one has a certain charm as the first large-language

00:11:37.186 --> 00:11:40.740
model to really capture the world's attention outside of ML communities.

00:11:41.440 --> 00:11:44.177
Also, practically speaking, companies tend to keep much tighter

00:11:44.177 --> 00:11:46.740
lips around the specific numbers for more modern networks.

00:11:47.360 --> 00:11:50.703
I just want to set the scene going in, that as you peek under the

00:11:50.703 --> 00:11:53.429
hood to see what happens inside a tool like ChatGPT,

00:11:53.429 --> 00:11:57.440
almost all of the actual computation looks like matrix vector multiplication.

00:11:57.900 --> 00:12:01.883
There's a little bit of a risk getting lost in the sea of billions of numbers,

00:12:01.883 --> 00:12:05.253
but you should draw a very sharp distinction in your mind between

00:12:05.253 --> 00:12:08.623
the weights of the model, which I'll always color in blue or red,

00:12:08.623 --> 00:12:11.840
and the data being processed, which I'll always color in gray.

00:12:12.180 --> 00:12:16.158
The weights are the actual brains, they are the things learned during training,

00:12:16.158 --> 00:12:17.920
and they determine how it behaves.

00:12:18.280 --> 00:12:22.299
The data being processed simply encodes whatever specific input is

00:12:22.299 --> 00:12:26.500
fed into the model for a given run, like an example snippet of text.

00:12:27.480 --> 00:12:31.702
With all of that as foundation, let's dig into the first step of this text processing

00:12:31.702 --> 00:12:35.973
example, which is to break up the input into little chunks and turn those chunks into

00:12:35.973 --> 00:12:36.420
vectors.

00:12:37.020 --> 00:12:39.261
I mentioned how those chunks are called tokens,

00:12:39.261 --> 00:12:41.501
which might be pieces of words or punctuation,

00:12:41.501 --> 00:12:44.886
but every now and then in this chapter and especially in the next one,

00:12:44.886 --> 00:12:48.080
I'd like to just pretend that it's broken more cleanly into words.

00:12:48.600 --> 00:12:51.386
Because we humans think in words, this will just make it much

00:12:51.386 --> 00:12:54.080
easier to reference little examples and clarify each step.

00:12:55.260 --> 00:12:59.420
The model has a predefined vocabulary, some list of all possible words,

00:12:59.420 --> 00:13:03.112
say 50,000 of them, and the first matrix that we'll encounter,

00:13:03.112 --> 00:13:07.800
known as the embedding matrix, has a single column for each one of these words.

00:13:08.940 --> 00:13:13.760
These columns are what determines what vector each word turns into in that first step.

00:13:15.100 --> 00:13:18.075
We label it W_E, and like all the matrices we see,

00:13:18.075 --> 00:13:22.360
its values begin random, but they're going to be learned based on data.

00:13:23.620 --> 00:13:27.378
Turning words into vectors was common practice in machine learning long before

00:13:27.378 --> 00:13:30.750
transformers, but it's a little weird if you've never seen it before,

00:13:30.750 --> 00:13:33.448
and it sets the foundation for everything that follows,

00:13:33.448 --> 00:13:35.760
so let's take a moment to get familiar with it.

00:13:36.040 --> 00:13:39.911
We often call this embedding a word, which invites you to think of these

00:13:39.911 --> 00:13:43.620
vectors very geometrically as points in some high dimensional space.

00:13:44.180 --> 00:13:48.054
Visualizing a list of three numbers as coordinates for points in 3D space would

00:13:48.054 --> 00:13:51.780
be no problem, but word embeddings tend to be much much higher dimensional.

00:13:52.280 --> 00:13:55.936
In GPT-3 they have 12,288 dimensions, and as you'll see,

00:13:55.936 --> 00:14:00.440
it matters to work in a space that has a lot of distinct directions.

00:14:01.180 --> 00:14:05.060
In the same way that you could take a two-dimensional slice through a 3D space

00:14:05.060 --> 00:14:08.791
and project all the points onto that slice, for the sake of animating word

00:14:08.791 --> 00:14:12.471
embeddings that a simple model is giving me, I'm going to do an analogous

00:14:12.471 --> 00:14:16.749
thing by choosing a three-dimensional slice through this very high dimensional space,

00:14:16.749 --> 00:14:20.480
and projecting the word vectors down onto that and displaying the results.

00:14:21.280 --> 00:14:25.522
The big idea here is that as a model tweaks and tunes its weights to determine

00:14:25.522 --> 00:14:28.730
how exactly words get embedded as vectors during training,

00:14:28.730 --> 00:14:33.026
it tends to settle on a set of embeddings where directions in the space have a

00:14:33.026 --> 00:14:34.440
kind of semantic meaning.

00:14:34.980 --> 00:14:37.790
For the simple word-to-vector model I'm running here,

00:14:37.790 --> 00:14:42.189
if I run a search for all the words whose embeddings are closest to that of tower,

00:14:42.189 --> 00:14:45.900
you'll notice how they all seem to give very similar tower-ish vibes.

00:14:46.340 --> 00:14:48.781
And if you want to pull up some Python and play along at home,

00:14:48.781 --> 00:14:51.380
this is the specific model that I'm using to make the animations.

00:14:51.620 --> 00:14:54.484
It's not a transformer, but it's enough to illustrate the

00:14:54.484 --> 00:14:57.600
idea that directions in the space can carry semantic meaning.

00:14:58.300 --> 00:15:02.187
A very classic example of this is how if you take the difference between

00:15:02.187 --> 00:15:05.750
the vectors for woman and man, something you would visualize as a

00:15:05.750 --> 00:15:09.961
little vector in the space connecting the tip of one to the tip of the other,

00:15:09.961 --> 00:15:13.200
it's very similar to the difference between king and queen.

00:15:15.080 --> 00:15:18.355
So let's say you didn't know the word for a female monarch,

00:15:18.355 --> 00:15:22.407
you could find it by taking king, adding this woman minus man direction,

00:15:22.407 --> 00:15:25.460
and searching for the embedding closest to that point.

00:15:27.000 --> 00:15:28.200
At least, kind of.

00:15:28.480 --> 00:15:31.773
Despite this being a classic example for the model I'm playing with,

00:15:31.773 --> 00:15:35.937
the true embedding of queen is actually a little farther off than this would suggest,

00:15:35.937 --> 00:15:39.957
presumably because the way queen is used in training data is not merely a feminine

00:15:39.957 --> 00:15:40.780
version of king.

00:15:41.620 --> 00:15:45.260
When I played around, family relations seemed to illustrate the idea much better.

00:15:46.340 --> 00:15:50.461
The point is, it looks like during training the model found it advantageous to

00:15:50.461 --> 00:15:54.900
choose embeddings such that one direction in this space encodes gender information.

00:15:56.800 --> 00:16:00.081
Another example is that if you take the embedding of Italy,

00:16:00.081 --> 00:16:04.753
and you subtract the embedding of Germany, and add that to the embedding of Hitler,

00:16:04.753 --> 00:16:08.090
you get something very close to the embedding of Mussolini.

00:16:08.570 --> 00:16:13.431
It's as if the model learned to associate some directions with Italian-ness,

00:16:13.431 --> 00:16:15.670
and others with WWII axis leaders.

00:16:16.470 --> 00:16:19.931
Maybe my favorite example in this vein is how in some models,

00:16:19.931 --> 00:16:24.187
if you take the difference between Germany and Japan, and add it to sushi,

00:16:24.187 --> 00:16:26.230
you end up very close to bratwurst.

00:16:27.350 --> 00:16:30.187
Also in playing this game of finding nearest neighbors,

00:16:30.187 --> 00:16:33.850
I was very pleased to see how close cat was to both beast and monster.

00:16:34.690 --> 00:16:37.743
One bit of mathematical intuition that's helpful to have in mind,

00:16:37.743 --> 00:16:40.703
especially for the next chapter, is how the dot product of two

00:16:40.703 --> 00:16:43.850
vectors can be thought of as a way to measure how well they align.

00:16:44.870 --> 00:16:47.693
Computationally, dot products involve multiplying all the

00:16:47.693 --> 00:16:51.111
corresponding components and then adding the results, which is good,

00:16:51.111 --> 00:16:54.330
since so much of our computation has to look like weighted sums.

00:16:55.190 --> 00:16:59.999
Geometrically, the dot product is positive when vectors point in similar directions,

00:16:59.999 --> 00:17:03.606
it's zero if they're perpendicular, and it's negative whenever

00:17:03.606 --> 00:17:05.610
they point in opposite directions.

00:17:06.550 --> 00:17:09.916
For example, let's say you were playing with this model,

00:17:09.916 --> 00:17:14.906
and you hypothesize that the embedding of cats minus cat might represent a sort of

00:17:14.906 --> 00:17:17.010
plurality direction in this space.

00:17:17.430 --> 00:17:20.570
To test this, I'm going to take this vector and compute its dot

00:17:20.570 --> 00:17:23.461
product against the embeddings of certain singular nouns,

00:17:23.461 --> 00:17:27.050
and compare it to the dot products with the corresponding plural nouns.

00:17:27.270 --> 00:17:30.219
If you play around with this, you'll notice that the plural ones

00:17:30.219 --> 00:17:33.628
do indeed seem to consistently give higher values than the singular ones,

00:17:33.628 --> 00:17:36.070
indicating that they align more with this direction.

00:17:37.070 --> 00:17:41.616
It's also fun how if you take this dot product with the embeddings of the words one,

00:17:41.616 --> 00:17:44.430
two, three, and so on, they give increasing values,

00:17:44.430 --> 00:17:49.030
so it's as if we can quantitatively measure how plural the model finds a given word.

00:17:50.250 --> 00:17:53.570
Again, the specifics for how words get embedded is learned using data.

00:17:54.050 --> 00:17:57.475
This embedding matrix, whose columns tell us what happens to each word,

00:17:57.475 --> 00:17:59.550
is the first pile of weights in our model.

00:18:00.030 --> 00:18:04.727
Using the GPT-3 numbers, the vocabulary size specifically is 50,257,

00:18:04.727 --> 00:18:09.770
and again, technically this consists not of words per se, but of tokens.

00:18:10.630 --> 00:18:14.309
The embedding dimension is 12,288, and multiplying those

00:18:14.309 --> 00:18:17.790
tells us this consists of about 617 million weights.

00:18:18.250 --> 00:18:20.626
Let's go ahead and add this to a running tally,

00:18:20.626 --> 00:18:23.810
remembering that by the end we should count up to 175 billion.

00:18:25.430 --> 00:18:28.756
In the case of transformers, you really want to think of the vectors

00:18:28.756 --> 00:18:32.130
in this embedding space as not merely representing individual words.

00:18:32.550 --> 00:18:36.513
For one thing, they also encode information about the position of that word,

00:18:36.513 --> 00:18:39.224
which we'll talk about later, but more importantly,

00:18:39.224 --> 00:18:42.770
you should think of them as having the capacity to soak in context.

00:18:43.350 --> 00:18:47.408
A vector that started its life as the embedding of the word king, for example,

00:18:47.408 --> 00:18:51.413
might progressively get tugged and pulled by various blocks in this network,

00:18:51.413 --> 00:18:55.575
so that by the end it points in a much more specific and nuanced direction that

00:18:55.575 --> 00:18:58.592
somehow encodes that it was a king who lived in Scotland,

00:18:58.592 --> 00:19:01.973
and who had achieved his post after murdering the previous king,

00:19:01.973 --> 00:19:04.730
and who's being described in Shakespearean language.

00:19:05.210 --> 00:19:07.790
Think about your own understanding of a given word.

00:19:08.250 --> 00:19:11.727
The meaning of that word is clearly informed by the surroundings,

00:19:11.727 --> 00:19:15.098
and sometimes this includes context from a long distance away,

00:19:15.098 --> 00:19:19.645
so in putting together a model that has the ability to predict what word comes next,

00:19:19.645 --> 00:19:23.390
the goal is to somehow empower it to incorporate context efficiently.

00:19:24.050 --> 00:19:27.136
To be clear, in that very first step, when you create the array of

00:19:27.136 --> 00:19:30.363
vectors based on the input text, each one of those is simply plucked

00:19:30.363 --> 00:19:33.496
out of the embedding matrix, so initially each one can only encode

00:19:33.496 --> 00:19:36.770
the meaning of a single word without any input from its surroundings.

00:19:37.710 --> 00:19:41.562
But you should think of the primary goal of this network that it flows through

00:19:41.562 --> 00:19:45.414
as being to enable each one of those vectors to soak up a meaning that's much

00:19:45.414 --> 00:19:48.970
more rich and specific than what mere individual words could represent.

00:19:49.510 --> 00:19:52.802
The network can only process a fixed number of vectors at a time,

00:19:52.802 --> 00:19:54.170
known as its context size.

00:19:54.510 --> 00:19:57.672
For GPT-3 it was trained with a context size of 2048,

00:19:57.672 --> 00:20:02.803
so the data flowing through the network always looks like this array of 2048 columns,

00:20:02.803 --> 00:20:05.010
each of which has 12,000 dimensions.

00:20:05.590 --> 00:20:08.657
This context size limits how much text the transformer can

00:20:08.657 --> 00:20:11.830
incorporate when it's making a prediction of the next word.

00:20:12.370 --> 00:20:15.042
This is why long conversations with certain chatbots,

00:20:15.042 --> 00:20:18.168
like the early versions of ChatGPT, often gave the feeling of

00:20:18.168 --> 00:20:22.050
the bot kind of losing the thread of conversation as you continued too long.

00:20:23.030 --> 00:20:25.230
We'll go into the details of attention in due time,

00:20:25.230 --> 00:20:28.810
but skipping ahead I want to talk for a minute about what happens at the very end.

00:20:29.450 --> 00:20:31.991
Remember, the desired output is a probability

00:20:31.991 --> 00:20:34.870
distribution over all tokens that might come next.

00:20:35.170 --> 00:20:37.759
For example, if the very last word is Professor,

00:20:37.759 --> 00:20:40.456
and the context includes words like Harry Potter,

00:20:40.456 --> 00:20:43.531
and immediately preceding we see least favorite teacher,

00:20:43.531 --> 00:20:47.685
and also if you give me some leeway by letting me pretend that tokens simply

00:20:47.685 --> 00:20:51.892
look like full words, then a well-trained network that had built up knowledge

00:20:51.892 --> 00:20:55.830
of Harry Potter would presumably assign a high number to the word Snape.

00:20:56.510 --> 00:20:57.970
This involves two different steps.

00:20:58.310 --> 00:21:03.052
The first one is to use another matrix that maps the very last vector in that

00:21:03.052 --> 00:21:07.610
context to a list of 50,000 values, one for each token in the vocabulary.

00:21:08.170 --> 00:21:12.173
Then there's a function that normalizes this into a probability distribution,

00:21:12.173 --> 00:21:15.657
it's called softmax and we'll talk more about it in just a second,

00:21:15.657 --> 00:21:19.868
but before that it might seem a little bit weird to only use this last embedding

00:21:19.868 --> 00:21:23.923
to make a prediction, when after all in that last step there are thousands of

00:21:23.923 --> 00:21:28.290
other vectors in the layer just sitting there with their own context-rich meanings.

00:21:28.930 --> 00:21:32.677
This has to do with the fact that in the training process it turns out to be

00:21:32.677 --> 00:21:36.424
much more efficient if you use each one of those vectors in the final layer

00:21:36.424 --> 00:21:40.270
to simultaneously make a prediction for what would come immediately after it.

00:21:40.970 --> 00:21:43.240
There's a lot more to be said about training later on,

00:21:43.240 --> 00:21:45.090
but I just want to call that out right now.

00:21:45.730 --> 00:21:49.690
This matrix is called the Unembedding matrix and we give it the label WU.

00:21:50.210 --> 00:21:53.574
Again, like all the weight matrices we see, its entries begin at random,

00:21:53.574 --> 00:21:55.910
but they are learned during the training process.

00:21:56.470 --> 00:21:59.447
Keeping score on our total parameter count, this Unembedding

00:21:59.447 --> 00:22:02.028
matrix has one row for each word in the vocabulary,

00:22:02.028 --> 00:22:05.650
and each row has the same number of elements as the embedding dimension.

00:22:06.410 --> 00:22:10.381
It's very similar to the embedding matrix, just with the order swapped,

00:22:10.381 --> 00:22:13.625
so it adds another 617 million parameters to the network,

00:22:13.625 --> 00:22:16.589
meaning our count so far is a little over a billion,

00:22:16.589 --> 00:22:20.224
a small but not wholly insignificant fraction of the 175 billion

00:22:20.224 --> 00:22:21.790
we'll end up with in total.

00:22:22.550 --> 00:22:24.657
As the very last mini-lesson for this chapter,

00:22:24.657 --> 00:22:26.901
I want to talk more about this softmax function,

00:22:26.901 --> 00:22:30.610
since it makes another appearance for us once we dive into the attention blocks.

00:22:31.430 --> 00:22:36.554
The idea is that if you want a sequence of numbers to act as a probability distribution,

00:22:36.554 --> 00:22:39.408
say a distribution over all possible next words,

00:22:39.408 --> 00:22:44.590
then each value has to be between 0 and 1, and you also need all of them to add up to 1.

00:22:45.250 --> 00:22:49.892
However, if you're playing the deep learning game where everything you do looks like

00:22:49.892 --> 00:22:54.810
matrix-vector multiplication, the outputs you get by default don't abide by this at all.

00:22:55.330 --> 00:22:57.785
The values are often negative, or much bigger than 1,

00:22:57.785 --> 00:22:59.870
and they almost certainly don't add up to 1.

00:23:00.510 --> 00:23:04.030
Softmax is the standard way to turn an arbitrary list of numbers

00:23:04.030 --> 00:23:08.705
into a valid distribution in such a way that the largest values end up closest to 1,

00:23:08.705 --> 00:23:11.290
and the smaller values end up very close to 0.

00:23:11.830 --> 00:23:13.070
That's all you really need to know.

00:23:13.090 --> 00:23:17.129
But if you're curious, the way it works is to first raise e to the power

00:23:17.129 --> 00:23:21.392
of each of the numbers, which means you now have a list of positive values,

00:23:21.392 --> 00:23:25.599
and then you can take the sum of all those positive values and divide each

00:23:25.599 --> 00:23:29.470
term by that sum, which normalizes it into a list that adds up to 1.

00:23:30.170 --> 00:23:34.286
You'll notice that if one of the numbers in the input is meaningfully bigger than the

00:23:34.286 --> 00:23:37.966
rest, then in the output the corresponding term dominates the distribution,

00:23:37.966 --> 00:23:42.131
so if you were sampling from it you'd almost certainly just be picking the maximizing

00:23:42.131 --> 00:23:42.470
input.

00:23:42.990 --> 00:23:46.998
But it's softer than just picking the max in the sense that when other values

00:23:46.998 --> 00:23:50.850
are similarly large, they also get meaningful weight in the distribution,

00:23:50.850 --> 00:23:54.650
and everything changes continuously as you continuously vary the inputs.

00:23:55.130 --> 00:23:59.984
In some situations, like when ChatGPT is using this distribution to create a next word,

00:23:59.984 --> 00:24:04.670
there's room for a little bit of extra fun by adding a little extra spice into this

00:24:04.670 --> 00:24:08.910
function, with a constant T thrown into the denominator of those exponents.

00:24:09.550 --> 00:24:14.000
We call it the temperature, since it vaguely resembles the role of temperature in

00:24:14.000 --> 00:24:18.121
certain thermodynamics equations, and the effect is that when T is larger,

00:24:18.121 --> 00:24:22.681
you give more weight to the lower values, meaning the distribution is a little bit

00:24:22.681 --> 00:24:26.911
more uniform, and if T is smaller, then the bigger values will dominate more

00:24:26.911 --> 00:24:31.526
aggressively, where in the extreme, setting T equal to zero means all of the weight

00:24:31.526 --> 00:24:32.790
goes to maximum value.

00:24:33.470 --> 00:24:37.662
For example, I'll have GPT-3 generate a story with the seed text,

00:24:37.662 --> 00:24:42.950
"once upon a time there was A", but I'll use different temperatures in each case.

00:24:43.630 --> 00:24:48.283
Temperature zero means that it always goes with the most predictable word,

00:24:48.283 --> 00:24:52.370
and what you get ends up being a trite derivative of Goldilocks.

00:24:53.010 --> 00:24:56.540
A higher temperature gives it a chance to choose less likely words,

00:24:56.540 --> 00:24:57.910
but it comes with a risk.

00:24:58.230 --> 00:25:01.148
In this case, the story starts out more originally,

00:25:01.148 --> 00:25:06.010
about a young web artist from South Korea, but it quickly degenerates into nonsense.

00:25:06.950 --> 00:25:10.830
Technically speaking, the API doesn't actually let you pick a temperature bigger than 2.

00:25:11.170 --> 00:25:15.336
There's no mathematical reason for this, it's just an arbitrary constraint imposed

00:25:15.336 --> 00:25:19.350
to keep their tool from being seen generating things that are too nonsensical.

00:25:19.870 --> 00:25:24.272
So if you're curious, the way this animation is actually working is I'm taking the

00:25:24.272 --> 00:25:27.011
20 most probable next tokens that GPT-3 generates,

00:25:27.011 --> 00:25:29.534
which seems to be the maximum they'll give me,

00:25:29.534 --> 00:25:32.970
and then I tweak the probabilities based on an exponent of 1/5.

00:25:33.130 --> 00:25:37.434
As another bit of jargon, in the same way that you might call the components of

00:25:37.434 --> 00:25:42.173
the output of this function probabilities, people often refer to the inputs as logits,

00:25:42.173 --> 00:25:46.150
or some people say logits, some people say logits, I'm gonna say logits.

00:25:46.530 --> 00:25:50.417
So for instance, when you feed in some text, you have all these word embeddings

00:25:50.417 --> 00:25:53.960
flow through the network, and you do this final multiplication with the

00:25:53.960 --> 00:25:58.241
unembedding matrix, machine learning people would refer to the components in that raw,

00:25:58.241 --> 00:26:01.390
unnormalized output as the logits for the next word prediction.

00:26:03.330 --> 00:26:06.697
A lot of the goal with this chapter was to lay the foundations for

00:26:06.697 --> 00:26:10.370
understanding the attention mechanism, Karate Kid wax-on-wax-off style.

00:26:10.850 --> 00:26:14.890
You see, if you have a strong intuition for word embeddings, for softmax,

00:26:14.890 --> 00:26:19.206
for how dot products measure similarity, and also the underlying premise that

00:26:19.206 --> 00:26:23.577
most of the calculations have to look like matrix multiplication with matrices

00:26:23.577 --> 00:26:27.562
full of tunable parameters, then understanding the attention mechanism,

00:26:27.562 --> 00:26:32.210
this cornerstone piece in the whole modern boom in AI, should be relatively smooth.

00:26:32.650 --> 00:26:34.510
For that, come join me in the next chapter.

00:26:36.390 --> 00:26:38.922
As I'm publishing this, a draft of that next chapter

00:26:38.922 --> 00:26:41.210
is available for review by Patreon supporters.

00:26:41.770 --> 00:26:44.239
A final version should be up in public in a week or two,

00:26:44.239 --> 00:26:47.370
it usually depends on how much I end up changing based on that review.

00:26:47.810 --> 00:26:49.708
In the meantime, if you want to dive into attention,

00:26:49.708 --> 00:26:52.410
and if you want to help the channel out a little bit, it's there waiting.

