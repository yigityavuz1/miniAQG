WEBVTT
Kind: captions
Language: en

00:00:04.180 --> 00:00:07.280
Last video I laid out the structure of a neural network.

00:00:07.680 --> 00:00:10.459
I'll give a quick recap here so that it's fresh in our minds,

00:00:10.459 --> 00:00:12.600
and then I have two main goals for this video.

00:00:13.100 --> 00:00:15.646
The first is to introduce the idea of gradient descent,

00:00:15.646 --> 00:00:18.054
which underlies not only how neural networks learn,

00:00:18.054 --> 00:00:20.600
but how a lot of other machine learning works as well.

00:00:21.120 --> 00:00:25.118
Then after that we'll dig in a little more into how this particular network performs,

00:00:25.118 --> 00:00:27.940
and what those hidden layers of neurons end up looking for.

00:00:28.980 --> 00:00:34.066
As a reminder, our goal here is the classic example of handwritten digit recognition,

00:00:34.066 --> 00:00:36.220
the hello world of neural networks.

00:00:37.020 --> 00:00:40.032
These digits are rendered on a 28x28 pixel grid,

00:00:40.032 --> 00:00:43.420
each pixel with some grayscale value between 0 and 1.

00:00:43.820 --> 00:00:50.040
Those are what determine the activations of 784 neurons in the input layer of the network.

00:00:51.180 --> 00:00:55.890
And then the activation for each neuron in the following layers is based on a weighted

00:00:55.890 --> 00:01:00.820
sum of all the activations in the previous layer, plus some special number called a bias.

00:01:02.160 --> 00:01:04.760
Then you compose that sum with some other function,

00:01:04.760 --> 00:01:08.940
like the sigmoid squishification, or a relu, the way I walked through last video.

00:01:09.480 --> 00:01:15.107
In total, given the somewhat arbitrary choice of two hidden layers with 16 neurons each,

00:01:15.107 --> 00:01:19.456
the network has about 13,000 weights and biases that we can adjust,

00:01:19.456 --> 00:01:24.380
and it's these values that determine what exactly the network actually does.

00:01:24.880 --> 00:01:29.090
Then what we mean when we say that this network classifies a given digit is that

00:01:29.090 --> 00:01:33.300
the brightest of those 10 neurons in the final layer corresponds to that digit.

00:01:34.100 --> 00:01:37.971
And remember, the motivation we had in mind here for the layered structure

00:01:37.971 --> 00:01:41.110
was that maybe the second layer could pick up on the edges,

00:01:41.110 --> 00:01:44.667
and the third layer might pick up on patterns like loops and lines,

00:01:44.667 --> 00:01:48.800
and the last one could just piece together those patterns to recognize digits.

00:01:49.800 --> 00:01:52.240
So here, we learn how the network learns.

00:01:52.640 --> 00:01:56.781
What we want is an algorithm where you can show this network a whole bunch of

00:01:56.781 --> 00:02:01.353
training data, which comes in the form of a bunch of different images of handwritten

00:02:01.353 --> 00:02:04.526
digits, along with labels for what they're supposed to be,

00:02:04.526 --> 00:02:08.937
and it'll adjust those 13,000 weights and biases so as to improve its performance

00:02:08.937 --> 00:02:10.120
on the training data.

00:02:10.720 --> 00:02:13.790
Hopefully, this layered structure will mean that what it

00:02:13.790 --> 00:02:16.860
learns generalizes to images beyond that training data.

00:02:17.640 --> 00:02:20.591
The way we test that is that after you train the network,

00:02:20.591 --> 00:02:23.645
you show it more labeled data that it's never seen before,

00:02:23.645 --> 00:02:26.700
and you see how accurately it classifies those new images.

00:02:31.120 --> 00:02:34.871
Fortunately for us, and what makes this such a common example to start with,

00:02:34.871 --> 00:02:39.264
is that the good people behind the MNIST database have put together a collection of tens

00:02:39.264 --> 00:02:43.410
of thousands of handwritten digit images, each one labeled with the numbers they're

00:02:43.410 --> 00:02:44.200
supposed to be.

00:02:44.900 --> 00:02:48.504
And as provocative as it is to describe a machine as learning,

00:02:48.504 --> 00:02:53.097
once you see how it works, it feels a lot less like some crazy sci-fi premise,

00:02:53.097 --> 00:02:55.480
and a lot more like a calculus exercise.

00:02:56.200 --> 00:02:59.960
I mean, basically it comes down to finding the minimum of a certain function.

00:03:01.940 --> 00:03:06.222
Remember, conceptually, we're thinking of each neuron as being connected to all

00:03:06.222 --> 00:03:10.558
the neurons in the previous layer, and the weights in the weighted sum defining

00:03:10.558 --> 00:03:14.244
its activation are kind of like the strengths of those connections,

00:03:14.244 --> 00:03:18.960
and the bias is some indication of whether that neuron tends to be active or inactive.

00:03:19.720 --> 00:03:22.171
And to start things off, we're just going to initialize

00:03:22.171 --> 00:03:24.400
all of those weights and biases totally randomly.

00:03:24.940 --> 00:03:27.873
Needless to say, this network is going to perform pretty horribly on

00:03:27.873 --> 00:03:30.720
a given training example, since it's just doing something random.

00:03:31.040 --> 00:03:36.020
For example, you feed in this image of a 3, and the output layer just looks like a mess.

00:03:36.600 --> 00:03:41.409
So what you do is define a cost function, a way of telling the computer,

00:03:41.409 --> 00:03:47.020
no, bad computer, that output should have activations which are 0 for most neurons,

00:03:47.020 --> 00:03:50.760
but 1 for this neuron, what you gave me is utter trash.

00:03:51.720 --> 00:03:56.438
To say that a little more mathematically, you add up the squares of the differences

00:03:56.438 --> 00:04:01.212
between each of those trash output activations and the value you want them to have,

00:04:01.212 --> 00:04:05.020
and this is what we'll call the cost of a single training example.

00:04:05.960 --> 00:04:11.472
Notice this sum is small when the network confidently classifies the image correctly,

00:04:11.472 --> 00:04:16.400
but it's large when the network seems like it doesn't know what it's doing.

00:04:18.640 --> 00:04:22.012
So then what you do is consider the average cost over all of

00:04:22.012 --> 00:04:25.440
the tens of thousands of training examples at your disposal.

00:04:27.040 --> 00:04:30.574
This average cost is our measure for how lousy the network is,

00:04:30.574 --> 00:04:32.740
and how bad the computer should feel.

00:04:33.420 --> 00:04:34.600
And that's a complicated thing.

00:04:35.040 --> 00:04:38.557
Remember how the network itself was basically a function,

00:04:38.557 --> 00:04:42.198
one that takes in 784 numbers as inputs, the pixel values,

00:04:42.198 --> 00:04:46.764
and spits out 10 numbers as its output, and in a sense it's parameterized

00:04:46.764 --> 00:04:48.800
by all these weights and biases?

00:04:49.500 --> 00:04:52.820
Well the cost function is a layer of complexity on top of that.

00:04:53.100 --> 00:04:56.789
It takes as its input those 13,000 or so weights and biases,

00:04:56.789 --> 00:05:01.646
and spits out a single number describing how bad those weights and biases are,

00:05:01.646 --> 00:05:06.564
and the way it's defined depends on the network's behavior over all the tens of

00:05:06.564 --> 00:05:08.900
thousands of pieces of training data.

00:05:09.520 --> 00:05:11.000
That's a lot to think about.

00:05:12.400 --> 00:05:15.820
But just telling the computer what a crappy job it's doing isn't very helpful.

00:05:16.220 --> 00:05:20.060
You want to tell it how to change those weights and biases so that it gets better.

00:05:20.780 --> 00:05:25.381
To make it easier, rather than struggling to imagine a function with 13,000 inputs,

00:05:25.381 --> 00:05:30.037
just imagine a simple function that has one number as an input and one number as an

00:05:30.037 --> 00:05:30.480
output.

00:05:31.480 --> 00:05:35.300
How do you find an input that minimizes the value of this function?

00:05:36.460 --> 00:05:41.169
Calculus students will know that you can sometimes figure out that minimum explicitly,

00:05:41.169 --> 00:05:44.728
but that's not always feasible for really complicated functions,

00:05:44.728 --> 00:05:49.437
certainly not in the 13,000 input version of this situation for our crazy complicated

00:05:49.437 --> 00:05:51.080
neural network cost function.

00:05:51.580 --> 00:05:54.578
A more flexible tactic is to start at any input,

00:05:54.578 --> 00:05:59.200
and figure out which direction you should step to make that output lower.

00:06:00.080 --> 00:06:04.092
Specifically, if you can figure out the slope of the function where you are,

00:06:04.092 --> 00:06:06.732
then shift to the left if that slope is positive,

00:06:06.732 --> 00:06:09.900
and shift the input to the right if that slope is negative.

00:06:11.960 --> 00:06:15.875
If you do this repeatedly, at each point checking the new slope and taking the

00:06:15.875 --> 00:06:19.840
appropriate step, you're going to approach some local minimum of the function.

00:06:20.640 --> 00:06:23.800
The image you might have in mind here is a ball rolling down a hill.

00:06:24.620 --> 00:06:27.791
Notice, even for this really simplified single input function,

00:06:27.791 --> 00:06:30.655
there are many possible valleys that you might land in,

00:06:30.655 --> 00:06:33.007
depending on which random input you start at,

00:06:33.007 --> 00:06:36.689
and there's no guarantee that the local minimum you land in is going to

00:06:36.689 --> 00:06:39.400
be the smallest possible value of the cost function.

00:06:40.220 --> 00:06:42.620
That will carry over to our neural network case as well.

00:06:43.180 --> 00:06:47.568
And I also want you to notice how if you make your step sizes proportional to the slope,

00:06:47.568 --> 00:06:50.511
then when the slope is flattening out towards the minimum,

00:06:50.511 --> 00:06:54.600
your steps get smaller and smaller, and that kind of helps you from overshooting.

00:06:55.940 --> 00:06:58.598
Bumping up the complexity a bit, imagine instead

00:06:58.598 --> 00:07:00.980
a function with two inputs and one output.

00:07:01.500 --> 00:07:04.497
You might think of the input space as the xy-plane,

00:07:04.497 --> 00:07:08.140
and the cost function as being graphed as a surface above it.

00:07:08.760 --> 00:07:11.803
Now instead of asking about the slope of the function,

00:07:11.803 --> 00:07:15.297
you have to ask which direction you should step in this input

00:07:15.297 --> 00:07:18.960
space so as to decrease the output of the function most quickly.

00:07:19.720 --> 00:07:21.760
In other words, what's the downhill direction?

00:07:22.380 --> 00:07:25.560
Again, it's helpful to think of a ball rolling down that hill.

00:07:26.660 --> 00:07:30.661
Those of you familiar with multivariable calculus will know that the

00:07:30.661 --> 00:07:34.603
gradient of a function gives you the direction of steepest ascent,

00:07:34.603 --> 00:07:38.780
which direction should you step to increase the function most quickly.

00:07:39.560 --> 00:07:42.800
Naturally enough, taking the negative of that gradient gives you

00:07:42.800 --> 00:07:46.040
the direction to step that decreases the function most quickly.

00:07:47.240 --> 00:07:50.569
Even more than that, the length of this gradient vector is

00:07:50.569 --> 00:07:53.840
an indication for just how steep that steepest slope is.

00:07:54.540 --> 00:07:57.611
If you're unfamiliar with multivariable calculus and want to learn more,

00:07:57.611 --> 00:08:00.340
check out some of the work I did for Khan Academy on the topic.

00:08:00.860 --> 00:08:04.485
Honestly though, all that matters for you and me right now is that

00:08:04.485 --> 00:08:07.561
in principle there exists a way to compute this vector,

00:08:07.561 --> 00:08:11.900
this vector that tells you what the downhill direction is and how steep it is.

00:08:12.400 --> 00:08:16.120
You'll be okay if that's all you know and you're not rock solid on the details.

00:08:17.200 --> 00:08:22.054
Cause If you can get that, the algorithm for minimizing the function is to compute this

00:08:22.054 --> 00:08:26.740
gradient direction, then take a small step downhill, and repeat that over and over.

00:08:27.700 --> 00:08:32.820
It's the same basic idea for a function that has 13,000 inputs instead of 2 inputs.

00:08:33.400 --> 00:08:36.596
Imagine organizing all 13,000 weights and biases

00:08:36.596 --> 00:08:39.460
of our network into a giant column vector.

00:08:40.140 --> 00:08:43.919
The negative gradient of the cost function is just a vector,

00:08:43.919 --> 00:08:48.896
it's some direction inside this insanely huge input space that tells you which

00:08:48.896 --> 00:08:53.683
nudges to all of those numbers is going to cause the most rapid decrease to

00:08:53.683 --> 00:08:54.880
the cost function.

00:08:55.640 --> 00:08:58.833
And of course, with our specially designed cost function,

00:08:58.833 --> 00:09:02.418
changing the weights and biases to decrease it means making the

00:09:02.418 --> 00:09:06.227
output of the network on each piece of training data look less like

00:09:06.227 --> 00:09:10.820
a random array of 10 values, and more like an actual decision we want it to make.

00:09:11.440 --> 00:09:15.877
It's important to remember, this cost function involves an average over all of the

00:09:15.877 --> 00:09:20.693
training data, so if you minimize it, it means it's a better performance on all of those

00:09:20.693 --> 00:09:21.180
samples.

00:09:23.820 --> 00:09:26.563
The algorithm for computing this gradient efficiently,

00:09:26.563 --> 00:09:29.764
which is effectively the heart of how a neural network learns,

00:09:29.764 --> 00:09:33.980
is called backpropagation, and it's what I'm going to be talking about next video.

00:09:34.660 --> 00:09:38.668
There, I really want to take the time to walk through what exactly happens to

00:09:38.668 --> 00:09:41.635
each weight and bias for a given piece of training data,

00:09:41.635 --> 00:09:45.903
trying to give an intuitive feel for what's happening beyond the pile of relevant

00:09:45.903 --> 00:09:47.100
calculus and formulas.

00:09:47.780 --> 00:09:50.780
Right here, right now, the main thing I want you to know,

00:09:50.780 --> 00:09:54.360
independent of implementation details, is that what we mean when we

00:09:54.360 --> 00:09:58.360
talk about a network learning is that it's just minimizing a cost function.

00:09:59.300 --> 00:10:03.651
And notice, one consequence of that is that it's important for this cost function to have

00:10:03.651 --> 00:10:07.611
a nice smooth output, so that we can find a local minimum by taking little steps

00:10:07.611 --> 00:10:08.100
downhill.

00:10:09.260 --> 00:10:13.889
This is why, by the way, artificial neurons have continuously ranging activations,

00:10:13.889 --> 00:10:17.333
rather than simply being active or inactive in a binary way,

00:10:17.333 --> 00:10:19.140
the way biological neurons are.

00:10:20.220 --> 00:10:23.567
This process of repeatedly nudging an input of a function by some

00:10:23.567 --> 00:10:26.760
multiple of the negative gradient is called gradient descent.

00:10:27.300 --> 00:10:30.837
It's a way to converge towards some local minimum of a cost function,

00:10:30.837 --> 00:10:32.580
basically a valley in this graph.

00:10:33.440 --> 00:10:36.885
I'm still showing the picture of a function with two inputs, of course,

00:10:36.885 --> 00:10:40.378
because nudges in a 13,000 dimensional input space are a little hard to

00:10:40.378 --> 00:10:44.260
wrap your mind around, but there is a nice non-spatial way to think about this.

00:10:45.080 --> 00:10:48.440
Each component of the negative gradient tells us two things.

00:10:49.060 --> 00:10:51.993
The sign, of course, tells us whether the corresponding

00:10:51.993 --> 00:10:55.140
component of the input vector should be nudged up or down.

00:10:55.800 --> 00:10:59.165
But importantly, the relative magnitudes of all these

00:10:59.165 --> 00:11:02.720
components kind of tells you which changes matter more.

00:11:05.220 --> 00:11:09.130
You see, in our network, an adjustment to one of the weights might have a much

00:11:09.130 --> 00:11:13.040
greater impact on the cost function than the adjustment to some other weight.

00:11:14.800 --> 00:11:18.200
Some of these connections just matter more for our training data.

00:11:19.320 --> 00:11:23.626
So a way you can think about this gradient vector of our mind-warpingly massive

00:11:23.626 --> 00:11:28.095
cost function is that it encodes the relative importance of each weight and bias,

00:11:28.095 --> 00:11:32.400
that is, which of these changes is going to carry the most bang for your buck.

00:11:33.620 --> 00:11:36.640
This really is just another way of thinking about direction.

00:11:37.100 --> 00:11:41.786
To take a simpler example, if you have some function with two variables as an input,

00:11:41.786 --> 00:11:46.082
and you compute that its gradient at some particular point comes out as 3,1,

00:11:46.082 --> 00:11:50.043
then on the one hand you can interpret that as saying that when you're

00:11:50.043 --> 00:11:55.008
standing at that input, moving along this direction increases the function most quickly,

00:11:55.008 --> 00:11:58.690
that when you graph the function above the plane of input points,

00:11:58.690 --> 00:12:02.260
that vector is what's giving you the straight uphill direction.

00:12:02.860 --> 00:12:07.355
But another way to read that is to say that changes to this first variable have 3

00:12:07.355 --> 00:12:10.463
times the importance as changes to the second variable,

00:12:10.463 --> 00:12:13.626
that at least in the neighborhood of the relevant input,

00:12:13.626 --> 00:12:16.900
nudging the x-value carries a lot more bang for your buck.

00:12:19.880 --> 00:12:22.340
Let's zoom out and sum up where we are so far.

00:12:22.840 --> 00:12:27.147
The network itself is this function with 784 inputs and 10 outputs,

00:12:27.147 --> 00:12:30.040
defined in terms of all these weighted sums.

00:12:30.640 --> 00:12:33.680
The cost function is a layer of complexity on top of that.

00:12:33.980 --> 00:12:37.850
It takes the 13,000 weights and biases as inputs and spits out

00:12:37.850 --> 00:12:41.720
a single measure of lousiness based on the training examples.

00:12:42.440 --> 00:12:46.900
And the gradient of the cost function is one more layer of complexity still.

00:12:47.360 --> 00:12:50.796
It tells us what nudges to all these weights and biases cause the

00:12:50.796 --> 00:12:53.439
fastest change to the value of the cost function,

00:12:53.439 --> 00:12:57.880
which you might interpret as saying which changes to which weights matter the most.

00:13:02.560 --> 00:13:06.089
So, when you initialize the network with random weights and biases,

00:13:06.089 --> 00:13:09.618
and adjust them many times based on this gradient descent process,

00:13:09.618 --> 00:13:13.200
how well does it actually perform on images it's never seen before?

00:13:14.100 --> 00:13:18.961
The one I've described here, with the two hidden layers of 16 neurons each,

00:13:18.961 --> 00:13:22.136
chosen mostly for aesthetic reasons, is not bad,

00:13:22.136 --> 00:13:25.960
classifying about 96% of the new images it sees correctly.

00:13:26.680 --> 00:13:30.164
And honestly, if you look at some of the examples it messes up on,

00:13:30.164 --> 00:13:32.540
you feel compelled to cut it a little slack.

00:13:36.220 --> 00:13:40.324
Now if you play around with the hidden layer structure and make a couple tweaks,

00:13:40.324 --> 00:13:41.760
you can get this up to 98%.

00:13:41.760 --> 00:13:42.720
And that's pretty good!

00:13:43.020 --> 00:13:47.851
It's not the best, you can certainly get better performance by getting more sophisticated

00:13:47.851 --> 00:13:52.030
than this plain vanilla network, but given how daunting the initial task is,

00:13:52.030 --> 00:13:56.698
I think there's something incredible about any network doing this well on images it's

00:13:56.698 --> 00:14:01.420
never seen before, given that we never specifically told it what patterns to look for.

00:14:02.560 --> 00:14:06.881
Originally, the way I motivated this structure was by describing a hope we might have,

00:14:06.881 --> 00:14:09.543
that the second layer might pick up on little edges,

00:14:09.543 --> 00:14:13.211
that the third layer would piece together those edges to recognize loops

00:14:13.211 --> 00:14:17.180
and longer lines, and that those might be pieced together to recognize digits.

00:14:17.960 --> 00:14:20.400
So is this what our network is actually doing?

00:14:21.080 --> 00:14:24.400
Well, for this one at least, not at all.

00:14:24.820 --> 00:14:28.917
Remember how last video we looked at how the weights of the connections from all

00:14:28.917 --> 00:14:32.809
the neurons in the first layer to a given neuron in the second layer can be

00:14:32.809 --> 00:14:37.060
visualized as a given pixel pattern that the second layer neuron is picking up on?

00:14:37.780 --> 00:14:42.622
Well, when we actually do that for the weights associated with these transitions,

00:14:42.622 --> 00:14:47.762
from the first layer to the next, instead of picking up on isolated little edges here

00:14:47.762 --> 00:14:52.843
and there, they look, well, almost random, just with some very loose patterns in the

00:14:52.843 --> 00:14:53.680
middle there.

00:14:53.760 --> 00:14:57.616
It would seem that in the unfathomably large 13,000 dimensional space

00:14:57.616 --> 00:15:01.248
of possible weights and biases, our network found itself a happy

00:15:01.248 --> 00:15:05.328
little local minimum that, despite successfully classifying most images,

00:15:05.328 --> 00:15:08.960
doesn't exactly pick up on the patterns we might have hoped for.

00:15:09.780 --> 00:15:13.820
And to really drive this point home, watch what happens when you input a random image.

00:15:14.320 --> 00:15:18.326
If the system was smart, you might expect it to feel uncertain,

00:15:18.326 --> 00:15:23.286
maybe not really activating any of those 10 output neurons or activating them

00:15:23.286 --> 00:15:27.801
all evenly, but instead it confidently gives you some nonsense answer,

00:15:27.801 --> 00:15:32.825
as if it feels as sure that this random noise is a 5 as it does that an actual

00:15:32.825 --> 00:15:34.160
image of a 5 is a 5.

00:15:34.540 --> 00:15:38.818
Phrased differently, even if this network can recognize digits pretty well,

00:15:38.818 --> 00:15:40.700
it has no idea how to draw them.

00:15:41.420 --> 00:15:45.240
A lot of this is because it's such a tightly constrained training setup.

00:15:45.880 --> 00:15:47.740
I mean, put yourself in the network's shoes here.

00:15:48.140 --> 00:15:52.378
From its point of view, the entire universe consists of nothing but clearly

00:15:52.378 --> 00:15:55.147
defined unmoving digits centered in a tiny grid,

00:15:55.147 --> 00:15:59.498
and its cost function never gave it any incentive to be anything but utterly

00:15:59.498 --> 00:16:01.080
confident in its decisions.

00:16:02.120 --> 00:16:05.373
So with this as the image of what those second layer neurons are really doing,

00:16:05.373 --> 00:16:07.918
you might wonder why I would introduce this network with the

00:16:07.918 --> 00:16:09.920
motivation of picking up on edges and patterns.

00:16:09.920 --> 00:16:12.300
I mean, that's just not at all what it ends up doing.

00:16:13.380 --> 00:16:17.180
Well, this is not meant to be our end goal, but instead a starting point.

00:16:17.640 --> 00:16:21.417
Frankly, this is old technology, the kind researched in the 80s and 90s,

00:16:21.417 --> 00:16:25.613
and you do need to understand it before you can understand more detailed modern

00:16:25.613 --> 00:16:29.495
variants, and it clearly is capable of solving some interesting problems,

00:16:29.495 --> 00:16:33.114
but the more you dig into what those hidden layers are really doing,

00:16:33.114 --> 00:16:34.740
the less intelligent it seems.

00:16:38.480 --> 00:16:42.337
Shifting the focus for a moment from how networks learn to how you learn,

00:16:42.337 --> 00:16:46.300
that'll only happen if you engage actively with the material here somehow.

00:16:47.060 --> 00:16:51.705
One pretty simple thing I want you to do is just pause right now and think deeply

00:16:51.705 --> 00:16:56.464
for a moment about what changes you might make to this system and how it perceives

00:16:56.464 --> 00:17:00.880
images if you wanted it to better pick up on things like edges and patterns.

00:17:01.480 --> 00:17:04.602
But better than that, to actually engage with the material,

00:17:04.602 --> 00:17:09.100
I highly recommend the book by Michael Nielsen on deep learning and neural networks.

00:17:09.680 --> 00:17:14.020
In it, you can find the code and the data to download and play with for this exact

00:17:14.020 --> 00:17:18.360
example, and the book will walk you through step by step what that code is doing.

00:17:19.300 --> 00:17:22.447
What's awesome is that this book is free and publicly available,

00:17:22.447 --> 00:17:26.726
so if you do get something out of it, consider joining me in making a donation towards

00:17:26.726 --> 00:17:27.660
Nielsen's efforts.

00:17:27.660 --> 00:17:31.625
I've also linked a couple other resources I like a lot in the description,

00:17:31.625 --> 00:17:36.018
including the phenomenal and beautiful blog post by Chris Ola and the articles in

00:17:36.018 --> 00:17:36.500
Distill.

00:17:38.280 --> 00:17:40.520
To close things off here for the last few minutes,

00:17:40.520 --> 00:17:43.880
I want to jump back into a snippet of the interview I had with Leisha Lee.

00:17:44.300 --> 00:17:47.720
You might remember her from the last video, she did her PhD work in deep learning.

00:17:48.300 --> 00:17:52.016
In this little snippet she talks about two recent papers that really dig into

00:17:52.016 --> 00:17:55.780
how some of the more modern image recognition networks are actually learning.

00:17:56.120 --> 00:17:58.462
Just to set up where we were in the conversation,

00:17:58.462 --> 00:18:02.621
the first paper took one of these particularly deep neural networks that's really good

00:18:02.621 --> 00:18:06.445
at image recognition, and instead of training it on a properly labeled dataset,

00:18:06.445 --> 00:18:08.740
shuffled all the labels around before training.

00:18:09.480 --> 00:18:13.315
Obviously the testing accuracy here was going to be no better than random,

00:18:13.315 --> 00:18:17.201
since everything's just randomly labeled. But it was still able to achieve

00:18:17.201 --> 00:18:20.880
the same training accuracy as you would on a properly labeled dataset.

00:18:21.600 --> 00:18:25.259
Basically, the millions of weights for this particular network were

00:18:25.259 --> 00:18:27.880
enough for it to just memorize the random data,

00:18:27.880 --> 00:18:31.594
which raises the question for whether minimizing this cost function

00:18:31.594 --> 00:18:36.400
actually corresponds to any sort of structure in the image, or is it just memorization?

00:18:51.440 --> 00:18:54.397
...to memorize the entire dataset of what the correct classification is.

00:18:54.397 --> 00:18:57.765
And so half a year later at ICML this year, there was not exactly rebuttal paper,

00:18:57.765 --> 00:18:59.901
but paper that addressed some aspects of like, hey,

00:18:59.901 --> 00:19:03.022
actually these networks are doing something a little bit smarter than that.

00:19:03.022 --> 00:19:06.431
If you look at that accuracy curve if you were just training on a random data set

00:19:06.431 --> 00:19:08.977
that curve went down very slowly, almost in a linear fashion.

00:19:08.977 --> 00:19:12.140
So youâ€™re really struggling to find that local minimum of the right weights.

00:19:12.240 --> 00:19:15.765
Whereas if you're actually training on a structured dataset,

00:19:15.765 --> 00:19:20.465
one that has the right labels, you fiddle around a little bit in the beginning,

00:19:20.465 --> 00:19:24.577
but then you kind of dropped very fast to get to that accuracy level,

00:19:24.577 --> 00:19:28.220
and so in some sense it was easier to find that local maxima.

00:19:28.540 --> 00:19:33.556
And so what was also interesting about that is it brings into light another paper from

00:19:33.556 --> 00:19:38.630
actually a couple of years ago, which has a lot more simplifications about the network

00:19:38.630 --> 00:19:43.821
layers, but one of the results was saying how if you look at the optimization landscape,

00:19:43.821 --> 00:19:48.604
the local minima that these networks tend to learn are actually of equal quality,

00:19:48.604 --> 00:19:51.404
so in some sense if your dataset is structured,

00:19:51.404 --> 00:19:54.320
you should be able to find that much more easily.

00:19:58.160 --> 00:20:01.180
My thanks, as always, to those of you supporting on Patreon.

00:20:01.520 --> 00:20:04.019
I've said before just what a game changer Patreon is,

00:20:04.019 --> 00:20:06.800
but these videos really would not be possible without you.

00:20:07.460 --> 00:20:10.120
I also want to give a special thanks to the VC firm Amplify Partners

00:20:10.120 --> 00:20:12.780
and their support of these initial videos in the series. Thank you.

